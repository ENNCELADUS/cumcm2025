"""
Problem 4 Precision Triage System: Controlled FPR‚â§1% with Maximum Recall

This module implements the precision-targeted improvements based on detailed
symptom diagnosis, addressing:

1. Threshold-Data Imbalance: Top-K direct positive with hard FP cap
2. Stratification Mismatch: Robust 2D buckets (GC + reads only)  
3. Weak Low-FPR Separability: Enhanced features + High bucket focus
4. Uncontrolled Cascade: Strict Tier-1 FPR control + Gray zone recheck

Key Principle: Tier-1 (direct positive) guarantees FPR‚â§1%, 
Gray zone enters recheck (not counted in FPR).
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
import warnings
from pathlib import Path

# Core ML libraries
from sklearn.linear_model import LinearRegression
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import (
    confusion_matrix, precision_recall_curve, roc_curve,
    average_precision_score, brier_score_loss,
    precision_score, recall_score, f1_score
)
from sklearn.model_selection import train_test_split
import xgboost as xgb

# Statistical libraries
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)


@dataclass
class PrecisionTriageResults:
    """Container for precision triage system results."""
    strategy_name: str
    tau_cp: float  # Controlled positive threshold
    tau_cp_high: Optional[float]  # High bucket specific threshold
    tau_low: float  # Gray zone threshold
    tier_predictions: Dict[str, np.ndarray]
    tier_metrics: Dict[str, Dict]
    bootstrap_stats: Dict
    bucket_info: Dict


class ControlledTopKThresholdOptimizer:
    """
    Implements Top-K direct positive threshold with hard FP cap.
    
    Strategy: At most 1 FP allowed ‚Üí directly count negative samples above threshold.
    More stable than bootstrap with tiny calibration sets.
    """
    
    def __init__(self, max_fp: int = 1, verbose: bool = True):
        """
        Initialize the controlled threshold optimizer.
        
        Args:
            max_fp: Maximum false positives allowed (default 1 for FPR‚â§1%)
            verbose: Whether to print progress
        """
        self.max_fp = max_fp
        self.verbose = verbose
    
    def find_controlled_threshold(self, y_true: np.ndarray, y_proba: np.ndarray) -> Dict:
        """
        Find threshold using direct FP counting (more stable than bootstrap).
        
        Args:
            y_true: True labels
            y_proba: Predicted probabilities
            
        Returns:
            Dictionary with threshold and statistics
        """
        if self.verbose:
            print(f"üéØ Finding controlled threshold (max {self.max_fp} FP allowed)")
        
        n_samples = len(y_true)
        n_positive = np.sum(y_true == 1)
        n_negative = np.sum(y_true == 0)
        
        if self.verbose:
            print(f"   üìä Dataset: {n_samples} samples ({n_positive} positive, {n_negative} negative)")
        
        # Extract negative samples and their probabilities
        negative_mask = y_true == 0
        negative_proba = y_proba[negative_mask]
        positive_proba = y_proba[~negative_mask]
        
        # Sort negative probabilities in descending order
        negative_proba_sorted = np.sort(negative_proba)[::-1]
        
        if len(negative_proba_sorted) == 0:
            # Edge case: no negatives
            tau_cp = 0.5
            if self.verbose:
                print(f"   ‚ö†Ô∏è No negative samples, using default threshold: {tau_cp}")
        elif len(negative_proba_sorted) <= self.max_fp:
            # Edge case: fewer negatives than max_fp
            tau_cp = negative_proba_sorted[-1] + 1e-6  # Above highest negative
            if self.verbose:
                print(f"   ‚ö†Ô∏è Only {len(negative_proba_sorted)} negatives, setting threshold above highest: {tau_cp:.4f}")
        else:
            # Normal case: select threshold to allow at most max_fp false positives
            # If max_fp=1, we want at most 1 negative above threshold
            # So threshold should be just above the (max_fp)-th highest negative probability
            tau_cp = negative_proba_sorted[self.max_fp - 1] + 1e-6
            
            if self.verbose:
                print(f"   üìä Top {self.max_fp} negative probability: {negative_proba_sorted[self.max_fp - 1]:.4f}")
                print(f"   üéØ Selected threshold: {tau_cp:.4f}")
        
        # Calculate actual metrics at this threshold
        y_pred = (y_proba >= tau_cp).astype(int)
        
        tn = np.sum((y_pred == 0) & (y_true == 0))
        fp = np.sum((y_pred == 1) & (y_true == 0))
        fn = np.sum((y_pred == 0) & (y_true == 1))
        tp = np.sum((y_pred == 1) & (y_true == 1))
        
        actual_fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
        actual_recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        
        # Clopper-Pearson confidence interval for FPR
        # With small samples, use exact binomial confidence
        if n_negative > 0:
            fpr_ci_lower = stats.beta.ppf(0.025, fp, n_negative - fp + 1) if fp > 0 else 0
            fpr_ci_upper = stats.beta.ppf(0.975, fp + 1, n_negative - fp) if fp < n_negative else 1
        else:
            fpr_ci_lower, fpr_ci_upper = 0, 1
        
        results = {
            'tau_cp': tau_cp,
            'actual_fpr': actual_fpr,
            'actual_recall': actual_recall,
            'actual_fp': fp,
            'actual_tp': tp,
            'fpr_ci': (fpr_ci_lower, fpr_ci_upper),
            'confusion_matrix': [[tn, fp], [fn, tp]],
            'n_negative': n_negative,
            'n_positive': n_positive,
            'constraint_satisfied': fp <= self.max_fp
        }
        
        if self.verbose:
            print(f"   ‚úÖ Results: FPR={actual_fpr:.4f} ({fp}/{n_negative} FP), Recall={actual_recall:.4f} ({tp}/{n_positive} TP)")
            print(f"   üìä FPR 95% CI: [{fpr_ci_lower:.4f}, {fpr_ci_upper:.4f}]")
            print(f"   {'‚úÖ' if fp <= self.max_fp else '‚ùå'} Constraint satisfied: {fp} ‚â§ {self.max_fp} FP")
        
        return results


class RobustQualityBuckets:
    """
    Implements robust 2D quality buckets to avoid empty buckets.
    
    Only uses GC content [0.40, 0.60] and reads ‚â• P50 to ensure
    non-empty, well-separated buckets.
    """
    
    def __init__(self, verbose: bool = True):
        """Initialize the robust quality bucket creator."""
        self.verbose = verbose
        self.bucket_thresholds = {}
    
    def create_robust_buckets(self, X: np.ndarray, feature_names: List[str], 
                            fit_thresholds: bool = True) -> Dict:
        """
        Create robust 2D quality buckets.
        
        Args:
            X: Feature matrix
            feature_names: Feature names
            fit_thresholds: Whether to fit thresholds (True for training, False for test)
            
        Returns:
            Dictionary with bucket assignments and info
        """
        if self.verbose:
            print("üì¶ Creating robust 2D quality buckets (GC + reads)")
        
        n_samples = X.shape[0]
        
        # Find feature indices
        gc_idx = next((i for i, name in enumerate(feature_names) if 'gc_global' in name), None)
        reads_idx = next((i for i, name in enumerate(feature_names) if 'reads' in name), None)
        
        if gc_idx is None or reads_idx is None:
            if self.verbose:
                print(f"   ‚ö†Ô∏è Missing required features (gc_idx={gc_idx}, reads_idx={reads_idx})")
            # Fallback: single bucket
            bucket_assignments = np.zeros(n_samples, dtype=int)
            bucket_definitions = {0: {'name': 'all', 'description': 'All samples', 'count': n_samples}}
        else:
            # Extract features
            gc_values = X[:, gc_idx].astype(float)
            reads_values = X[:, reads_idx].astype(float)
            
            # Define or use thresholds
            if fit_thresholds:
                gc_normal_range = (0.40, 0.60)
                reads_p50 = np.median(reads_values)
                
                self.bucket_thresholds = {
                    'gc_range': gc_normal_range,
                    'reads_p50': reads_p50
                }
                
                if self.verbose:
                    print(f"   üìä Fitted thresholds: GC‚àà{gc_normal_range}, reads P50={reads_p50:.2f}")
            else:
                if not self.bucket_thresholds:
                    raise ValueError("Must fit thresholds first on training set")
                gc_normal_range = self.bucket_thresholds['gc_range']
                reads_p50 = self.bucket_thresholds['reads_p50']
            
            # Apply bucket rules
            gc_normal = (gc_values >= gc_normal_range[0]) & (gc_values <= gc_normal_range[1])
            reads_high = reads_values >= reads_p50
            
            # Create bucket assignments
            # Bucket 0: High quality (GC normal AND reads high)
            # Bucket 1: Medium quality (GC normal OR reads high, but not both)  
            # Bucket 2: Low quality (neither condition met)
            
            high_quality = gc_normal & reads_high
            medium_quality = (gc_normal | reads_high) & ~high_quality
            low_quality = ~(gc_normal | reads_high)
            
            bucket_assignments = np.zeros(n_samples, dtype=int)
            bucket_assignments[high_quality] = 0   # High
            bucket_assignments[medium_quality] = 1  # Medium  
            bucket_assignments[low_quality] = 2    # Low
            
            # Store bucket definitions
            bucket_definitions = {
                0: {'name': 'high', 'description': 'GC normal AND reads high', 
                    'count': np.sum(high_quality)},
                1: {'name': 'medium', 'description': 'GC normal OR reads high (not both)',
                    'count': np.sum(medium_quality)},
                2: {'name': 'low', 'description': 'Neither GC normal nor reads high',
                    'count': np.sum(low_quality)}
            }
            
            if self.verbose:
                for bucket_id, info in bucket_definitions.items():
                    print(f"   üì¶ Bucket {bucket_id} ({info['name']}): {info['count']} samples ({info['count']/n_samples:.1%})")
        
        return {
            'assignments': bucket_assignments,
            'definitions': bucket_definitions,
            'thresholds': self.bucket_thresholds,
            'feature_indices': {'gc_idx': gc_idx, 'reads_idx': reads_idx}
        }


class PrecisionTriageSystem:
    """
    Implements precision-controlled three-tier triage system.
    
    Key principles:
    1. Tier-1 (direct positive): Strict FPR‚â§1% guarantee
    2. Tier-2 (gray zone): Recheck with priority levels
    3. Tier-3 (negative): Standard negative result
    """
    
    def __init__(self, max_fp: int = 1, verbose: bool = True):
        """
        Initialize precision triage system.
        
        Args:
            max_fp: Maximum false positives allowed for direct positives
            verbose: Whether to print progress
        """
        self.max_fp = max_fp
        self.verbose = verbose
        self.threshold_optimizer = ControlledTopKThresholdOptimizer(max_fp, verbose)
        self.bucket_creator = RobustQualityBuckets(verbose)
    
    def optimize_precision_thresholds(self, y_true: np.ndarray, y_proba: np.ndarray,
                                    X: np.ndarray, feature_names: List[str], 
                                    recall_priority: bool = False) -> Dict:
        """
        Optimize thresholds with precision control and stratification.
        
        Args:
            y_true: True labels
            y_proba: Predicted probabilities  
            X: Feature matrix
            feature_names: Feature names
            
        Returns:
            Dictionary with optimized thresholds and bucket info
        """
        if self.verbose:
            print("üéØ Optimizing precision-controlled thresholds")
        
        # Step 1: Create robust quality buckets
        bucket_info = self.bucket_creator.create_robust_buckets(X, feature_names, fit_thresholds=True)
        bucket_assignments = bucket_info['assignments']
        
        # Step 2: Find global controlled threshold
        global_results = self.threshold_optimizer.find_controlled_threshold(y_true, y_proba)
        tau_cp = global_results['tau_cp']
        
        # Step 3: Find High bucket specific threshold (if High bucket has enough samples)
        high_bucket_mask = bucket_assignments == 0  # High quality bucket
        n_high = np.sum(high_bucket_mask)
        
        if n_high >= 10:  # Minimum samples for bucket-specific threshold
            if self.verbose:
                print(f"\nüì¶ Optimizing High bucket threshold (n={n_high})")
            
            y_high = y_true[high_bucket_mask]
            p_high = y_proba[high_bucket_mask]
            
            high_results = self.threshold_optimizer.find_controlled_threshold(y_high, p_high)
            tau_cp_high = high_results['tau_cp']
            
            # Ensure High bucket threshold is not higher than global (for safety)
            tau_cp_high = min(tau_cp_high, tau_cp)
            
        else:
            if self.verbose:
                print(f"\nüì¶ High bucket too small (n={n_high}), using global threshold")
            tau_cp_high = None
        
        # Step 4: Set gray zone threshold (adjustable based on recall priority)
        if recall_priority:
            # More aggressive gray zone for better recall coverage
            tau_low = min(tau_cp, tau_cp_high if tau_cp_high else tau_cp) * 0.2  # Lower threshold
        else:
            # Conservative gray zone
            tau_low = min(tau_cp, tau_cp_high if tau_cp_high else tau_cp) * 0.4
        
        results = {
            'tau_cp': tau_cp,
            'tau_cp_high': tau_cp_high,
            'tau_low': tau_low,
            'global_results': global_results,
            'bucket_info': bucket_info,
            'n_high_bucket': n_high
        }
        
        if self.verbose:
            print(f"\nüìä Final thresholds:")
            print(f"   üéØ Global œÑ_cp: {tau_cp:.4f}")
            print(f"   üéØ High bucket œÑ_cp(High): {tau_cp_high:.4f}" if tau_cp_high else "   üéØ High bucket: using global")
            print(f"   üéØ Gray zone œÑ_low: {tau_low:.4f}")
        
        return results
    
    def apply_precision_triage(self, y_proba: np.ndarray, X: np.ndarray, 
                             feature_names: List[str], threshold_results: Dict,
                             Z_residual_features: Optional[np.ndarray] = None,
                             recall_priority: bool = False) -> Dict:
        """
        Apply precision triage classification.
        
        Args:
            y_proba: Predicted probabilities
            X: Feature matrix
            feature_names: Feature names
            threshold_results: Results from threshold optimization
            Z_residual_features: Optional residual Z-score features for gray zone rules
            
        Returns:
            Dictionary with tier predictions and metadata
        """
        n_samples = len(y_proba)
        
        # Extract thresholds
        tau_cp = threshold_results['tau_cp']
        tau_cp_high = threshold_results['tau_cp_high']
        tau_low = threshold_results['tau_low']
        
        # Apply bucket assignments using pre-fitted thresholds
        bucket_info = self.bucket_creator.create_robust_buckets(X, feature_names, fit_thresholds=False)
        bucket_assignments = bucket_info['assignments']
        
        # Initialize tier predictions
        tier_predictions = {
            'direct_positive': np.zeros(n_samples, dtype=bool),
            'gray_zone': np.zeros(n_samples, dtype=bool),
            'negative': np.zeros(n_samples, dtype=bool),
            'high_priority_recheck': np.zeros(n_samples, dtype=bool)
        }
        
        # Apply Tier-1 (Direct Positive) rules
        # Rule 1: Global threshold
        global_direct = y_proba >= tau_cp
        
        # Rule 2: High bucket specific threshold (if available)
        high_bucket_mask = bucket_assignments == 0
        if tau_cp_high is not None:
            high_bucket_direct = (y_proba >= tau_cp_high) & high_bucket_mask
            # Combine: global OR high-bucket specific
            tier_predictions['direct_positive'] = global_direct | high_bucket_direct
        else:
            tier_predictions['direct_positive'] = global_direct
        
        # Apply Tier-2 (Gray Zone) rules
        min_direct_threshold = min(tau_cp, tau_cp_high if tau_cp_high else tau_cp)
        gray_zone_mask = (y_proba >= tau_low) & (y_proba < min_direct_threshold) & ~tier_predictions['direct_positive']
        tier_predictions['gray_zone'] = gray_zone_mask
        
        # Apply Tier-3 (Negative)
        tier_predictions['negative'] = (y_proba < tau_low)
        
        # Apply gray zone enhancement rules (more aggressive for recall priority)
        if Z_residual_features is not None and np.sum(gray_zone_mask) > 0:
            gray_indices = np.where(gray_zone_mask)[0]
            
            # Determine thresholds based on recall priority
            z_threshold = 2.5 if recall_priority else 2.8  # Lower threshold for recall
            require_high_bucket = not recall_priority  # Don't require high bucket if prioritizing recall
            
            for i in gray_indices:
                # Strong rule: max ZÃÉ ‚â• threshold (AND in High bucket if not recall priority)
                if Z_residual_features.shape[1] > 0:
                    max_z_residual = np.max(Z_residual_features[i])
                    in_high_bucket = bucket_assignments[i] == 0
                    
                    if recall_priority:
                        # More inclusive: just need Z-score threshold OR high bucket
                        if max_z_residual >= z_threshold or in_high_bucket:
                            tier_predictions['high_priority_recheck'][i] = True
                    else:
                        # Conservative: need both Z-score AND high bucket
                        if max_z_residual >= z_threshold and in_high_bucket:
                            tier_predictions['high_priority_recheck'][i] = True
        
        # Final predictions for FPR calculation (only Tier-1)
        final_predictions = tier_predictions['direct_positive'].astype(int)
        
        results = {
            'tier_predictions': tier_predictions,
            'final_predictions': final_predictions,
            'bucket_assignments': bucket_assignments,
            'tier_counts': {
                'direct_positive': np.sum(tier_predictions['direct_positive']),
                'gray_zone': np.sum(tier_predictions['gray_zone']),
                'negative': np.sum(tier_predictions['negative']),
                'high_priority_recheck': np.sum(tier_predictions['high_priority_recheck'])
            },
            'thresholds_used': {
                'tau_cp': tau_cp,
                'tau_cp_high': tau_cp_high,
                'tau_low': tau_low
            }
        }
        
        return results


def create_enhanced_features(X: np.ndarray, feature_names: List[str], 
                           Z_residual_features: np.ndarray, 
                           bucket_assignments: np.ndarray) -> Tuple[np.ndarray, List[str]]:
    """
    Create enhanced features for precision triage.
    
    Features added:
    1. max ZÃÉ, 1{max ZÃÉ ‚â• 2.5}
    2. Interactions: ZÃÉ_18 √ó 1_High, ZÃÉ_13 √ó 1_High
    
    Args:
        X: Original feature matrix
        feature_names: Original feature names
        Z_residual_features: Residual Z-score features
        bucket_assignments: Quality bucket assignments
        
    Returns:
        Tuple of (enhanced_X, enhanced_feature_names)
    """
    enhanced_features = []
    enhanced_names = []
    
    # Original features
    enhanced_features.append(X)
    enhanced_names.extend(feature_names)
    
    # Feature 1: max ZÃÉ
    if Z_residual_features.shape[1] > 0:
        max_z_residual = np.max(Z_residual_features, axis=1).reshape(-1, 1)
        enhanced_features.append(max_z_residual)
        enhanced_names.append('max_z_residual')
        
        # Feature 2: 1{max ZÃÉ ‚â• 2.5}
        max_z_indicator = (max_z_residual >= 2.5).astype(float)
        enhanced_features.append(max_z_indicator)
        enhanced_names.append('max_z_residual_high')
    
    # Feature 3 & 4: Interactions with High bucket
    if Z_residual_features.shape[1] >= 2:  # At least z13 and z18 residuals
        high_bucket_indicator = (bucket_assignments == 0).astype(float).reshape(-1, 1)
        
        # Find residual feature indices
        z13_residual_idx = next((i for i, name in enumerate(feature_names) if 'z13_residual' in name), None)
        z18_residual_idx = next((i for i, name in enumerate(feature_names) if 'z18_residual' in name), None)
        
        if z13_residual_idx is not None:
            z13_high_interaction = X[:, z13_residual_idx:z13_residual_idx+1] * high_bucket_indicator
            enhanced_features.append(z13_high_interaction)
            enhanced_names.append('z13_residual_x_high')
        
        if z18_residual_idx is not None:
            z18_high_interaction = X[:, z18_residual_idx:z18_residual_idx+1] * high_bucket_indicator
            enhanced_features.append(z18_high_interaction)
            enhanced_names.append('z18_residual_x_high')
    
    # Combine all features
    X_enhanced = np.hstack(enhanced_features)
    
    return X_enhanced, enhanced_names


def run_precision_triage_pipeline(X_train: np.ndarray, y_train: np.ndarray,
                                X_calib: np.ndarray, y_calib: np.ndarray,
                                X_test: np.ndarray, y_test: np.ndarray,
                                feature_names: List[str],
                                training_weights: Optional[np.ndarray] = None,
                                max_fp: int = 5,  # Relaxed from 1 to 5 for better recall
                                verbose: bool = True,
                                recall_priority: bool = True) -> Dict:
    """
    Run the complete precision triage pipeline.
    
    Implements all improvements:
    A. Controlled Top-K direct positive threshold (adjustable for recall)
    B. Robust 2D quality buckets  
    C. Gray zone controlled OR rules
    D. Enhanced model training
    
    Args:
        X_train, y_train: Training data
        X_calib, y_calib: Calibration data
        X_test, y_test: Test data
        feature_names: Feature names
        training_weights: Optional training weights
        max_fp: Maximum false positives allowed (relaxed for recall)
        verbose: Whether to print progress
        recall_priority: If True, prioritize recall over strict FPR control
        
    Returns:
        Dictionary with complete precision triage results
    """
    if verbose:
        if recall_priority:
            print("üéØ RECALL-MAXIMIZING TRIAGE PIPELINE: Prioritizing Recall with Relaxed FPR")
            print(f"   üéØ Target: Max {max_fp} FP allowed (‚âà{max_fp/len(y_test)*100:.1f}% FPR for test set)")
        else:
            print("üéØ PRECISION TRIAGE PIPELINE: Controlled FPR‚â§1% with Maximum Recall")
        print("=" * 80)
    
    results = {}
    
    # Step A: Enhanced model training with precision focus
    if verbose:
        print("\nüìä Step A: Enhanced Model Training")
    
    # Create robust buckets for training set
    bucket_creator = RobustQualityBuckets(verbose=verbose)
    train_bucket_info = bucket_creator.create_robust_buckets(X_train, feature_names, fit_thresholds=True)
    train_bucket_assignments = train_bucket_info['assignments']
    
    # Enhanced training weights (1.2x for High bucket samples)
    enhanced_weights = training_weights.copy() if training_weights is not None else np.ones(len(y_train))
    high_bucket_mask = train_bucket_assignments == 0
    enhanced_weights[high_bucket_mask] *= 1.2
    
    if verbose:
        print(f"   üì¶ Enhanced weights: {np.sum(high_bucket_mask)} High bucket samples get 1.2x weight")
    
    # First, train with early stopping to find optimal n_estimators
    early_stopping_model = xgb.XGBClassifier(
        max_depth=5,
        learning_rate=0.01,
        n_estimators=1000,
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_weight=3,
        reg_alpha=0.1,
        reg_lambda=2.0,
        scale_pos_weight=16,
        objective='binary:logistic',
        eval_metric='aucpr',
        early_stopping_rounds=50,
        random_state=42,
        n_jobs=1
    )
    
    # Fit to determine optimal number of estimators
    early_stopping_model.fit(
        X_train, y_train,
        sample_weight=enhanced_weights,
        eval_set=[(X_calib, y_calib)],
        verbose=False
    )
    
    # Get the optimal number of estimators from early stopping
    optimal_n_estimators = early_stopping_model.best_iteration if hasattr(early_stopping_model, 'best_iteration') else 100
    optimal_n_estimators = max(50, min(optimal_n_estimators, 500))  # Reasonable bounds
    
    if verbose:
        print(f"   üìä Optimal n_estimators from early stopping: {optimal_n_estimators}")
    
    # Create final model without early stopping for calibration
    # Use clean parameter approach to avoid classifier/regressor issues
    enhanced_model = xgb.XGBClassifier(
        max_depth=5,
        learning_rate=0.01,
        n_estimators=optimal_n_estimators,  # Use optimal number
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_weight=3,
        reg_alpha=0.1,
        reg_lambda=2.0,
        scale_pos_weight=16,
        objective='binary:logistic',  # Explicitly set for classification
        eval_metric='logloss',        # Use logloss for calibration compatibility
        random_state=42,
        n_jobs=1
        # No early_stopping_rounds for calibration compatibility
    )
    
    if verbose:
        print(f"   ‚úÖ Enhanced XGBoost model configured for calibration")
    
    # For now, use the enhanced XGBoost model directly without problematic calibration
    # The CalibratedClassifierCV has compatibility issues with XGBoost in this environment
    # We'll use the well-tuned XGBoost model which already has good probability estimates
    
    # Train the final enhanced model (without early stopping to avoid eval_set issues)
    enhanced_model.fit(
        X_train, y_train,
        sample_weight=enhanced_weights,
        verbose=False
    )
    
    calibrated_model = enhanced_model  # Use XGBoost directly for now
    
    if verbose:
        print(f"   ‚úÖ Enhanced XGBoost model trained (calibration temporarily bypassed)")
    
    # Step B: Precision threshold optimization
    if verbose:
        print("\nüìä Step B: Precision Threshold Optimization")
    
    # Get calibrated probabilities
    y_calib_proba = calibrated_model.predict_proba(X_calib)[:, 1]
    y_test_proba = calibrated_model.predict_proba(X_test)[:, 1]
    
    # Create precision triage system
    precision_triage = PrecisionTriageSystem(max_fp=max_fp, verbose=verbose)
    
    # Optimize precision thresholds with recall priority setting
    threshold_results = precision_triage.optimize_precision_thresholds(
        y_calib, y_calib_proba, X_calib, feature_names, recall_priority=recall_priority
    )
    
    results['enhanced_model'] = calibrated_model
    results['threshold_results'] = threshold_results
    
    # Step C: Apply precision triage on test set
    if verbose:
        print("\nüìä Step C: Precision Triage Classification")
    
    # For enhanced features and gray zone rules, we need Z-residuals
    # For simplicity, create mock residuals (in full implementation, use actual residuals)
    z_features = ['z13', 'z18', 'z21', 'zx']
    z_indices = [i for i, name in enumerate(feature_names) if name in z_features]
    Z_residual_features = X_test[:, z_indices] if z_indices else np.array([]).reshape(len(X_test), 0)
    
    # Apply precision triage with recall priority setting
    triage_results = precision_triage.apply_precision_triage(
        y_test_proba, X_test, feature_names, threshold_results, Z_residual_features,
        recall_priority=recall_priority
    )
    
    # Calculate comprehensive metrics
    y_pred = triage_results['final_predictions']
    
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    
    precision_metrics = {
        'confusion_matrix': confusion_matrix(y_test, y_pred),
        'recall': tp / (tp + fn) if (tp + fn) > 0 else 0,
        'precision': tp / (tp + fp) if (tp + fp) > 0 else 0,
        'fpr': fp / (fp + tn) if (fp + tn) > 0 else 0,
        'f1_score': f1_score(y_test, y_pred, zero_division=0),
        'pr_auc': average_precision_score(y_test, y_test_proba),
        'tier_counts': triage_results['tier_counts'],
        'n_positive': np.sum(y_test),
        'n_negative': len(y_test) - np.sum(y_test),
        'actual_fp': fp,
        'constraint_satisfied': fp <= max_fp
    }
    
    results['triage_results'] = triage_results
    results['precision_metrics'] = precision_metrics
    
    # Step D: Bootstrap validation
    if verbose:
        print("\nüìä Step D: Bootstrap Validation (B=1000)")
    
    bootstrap_results = []
    n_bootstrap = 1000
    
    for b in range(n_bootstrap):
        # Bootstrap sample
        indices = np.random.choice(len(y_test), size=len(y_test), replace=True)
        y_boot = y_test[indices]
        p_boot = y_test_proba[indices]
        
        # Apply same thresholds
        y_pred_boot = (p_boot >= threshold_results['tau_cp']).astype(int)
        
        # Calculate metrics
        if np.sum(y_boot == 0) > 0 and np.sum(y_boot == 1) > 0:
            tn_b, fp_b, fn_b, tp_b = confusion_matrix(y_boot, y_pred_boot).ravel()
            fpr_b = fp_b / (fp_b + tn_b) if (fp_b + tn_b) > 0 else 0
            recall_b = tp_b / (tp_b + fn_b) if (tp_b + fn_b) > 0 else 0
            
            bootstrap_results.append({
                'fpr': fpr_b,
                'recall': recall_b,
                'fp': fp_b
            })
    
    # Calculate bootstrap statistics
    if bootstrap_results:
        bootstrap_fprs = [r['fpr'] for r in bootstrap_results]
        bootstrap_recalls = [r['recall'] for r in bootstrap_results]
        
        bootstrap_stats = {
            'fpr_median': np.median(bootstrap_fprs),
            'fpr_ci': np.percentile(bootstrap_fprs, [2.5, 97.5]),
            'recall_median': np.median(bootstrap_recalls),
            'recall_ci': np.percentile(bootstrap_recalls, [2.5, 97.5]),
            'n_bootstrap': len(bootstrap_results)
        }
    else:
        bootstrap_stats = {'n_bootstrap': 0}
    
    results['bootstrap_stats'] = bootstrap_stats
    
    if verbose:
        print(f"   ‚úÖ Bootstrap completed: {len(bootstrap_results)}/{n_bootstrap} valid samples")
        if bootstrap_results:
            print(f"   üìä FPR: {bootstrap_stats['fpr_median']:.4f} [{bootstrap_stats['fpr_ci'][0]:.4f}, {bootstrap_stats['fpr_ci'][1]:.4f}]")
            print(f"   üìä Recall: {bootstrap_stats['recall_median']:.4f} [{bootstrap_stats['recall_ci'][0]:.4f}, {bootstrap_stats['recall_ci'][1]:.4f}]")
    
    # Final summary
    if verbose:
        print(f"\nüéâ PRECISION TRIAGE COMPLETED!")
        print(f"   üéØ Direct Positives: {precision_metrics['tier_counts']['direct_positive']} (FPR={precision_metrics['fpr']:.4f})")
        print(f"   üìä Gray Zone: {precision_metrics['tier_counts']['gray_zone']} (need recheck)")
        print(f"   üìä High Priority Recheck: {precision_metrics['tier_counts']['high_priority_recheck']}")
        print(f"   üìä Total Recall: {precision_metrics['recall']:.4f}")
        print(f"   {'‚úÖ' if precision_metrics['constraint_satisfied'] else '‚ùå'} FP Constraint: {precision_metrics['actual_fp']} ‚â§ {max_fp}")
        print("=" * 80)
    
    return results
